from pipeline.preprocess_pdfs import Preprocessing, CustomPreprocessingGDPR, CustomPreprocessingEUAI
from config.chroma_client import ChromadDBConfig
import asyncio
import os

async def run_basic_preprocessing(pdfs: list[str]):
    print("\n--- ğŸš€ START: Basic Preprocessing ---")
    all_chunks = []

    for pdf_path in pdfs:
        if os.path.exists(pdf_path):
            print(f"ğŸ“„ Processing: {pdf_path}")
            chunks = await Preprocessing.preprocess_pdfs_chroma(pdf_path)
            all_chunks.extend(chunks)
        else:
            print(f"âš ï¸ Warning: File not found: {pdf_path}")

    if all_chunks:
        print(f"ğŸ“Š Total Basic chunks generated: {len(all_chunks)}")
        collection_name = "gdpr_euAI_complainces_basic_preprocess"
        chroma_client = ChromadDBConfig(collection_name=collection_name)
        
        # Explicitly initialize before adding
        await chroma_client.get_vectorstore()
        await chroma_client.add_documents(all_chunks)
    else:
        print("âš ï¸ No chunks to add for Basic Preprocessing.")
    print("--- ğŸ END: Basic Preprocessing ---\n")

async def run_custom_preprocessing_gdpr(jsonl: list[str]):
    print("\n--- ğŸš€ START: GDPR Preprocessing ---")
    all_chunks = []

    for jsonl_path in jsonl:
        if os.path.exists(jsonl_path):
            print(f"ğŸ“„ Processing: {jsonl_path}")
            chunks = await CustomPreprocessingGDPR.preprocess_gdpr_to_chunks(jsonl_path)
            all_chunks.extend(chunks)
        else:
            print(f"âš ï¸ Warning: File not found: {jsonl_path}")

    if all_chunks:
        print(f"ğŸ“Š Total GDPR chunks generated: {len(all_chunks)}")
        collection_name = "gdpr_euAI_complainces_custom_preprocess"
        chroma_client = ChromadDBConfig(collection_name=collection_name)
        
        await chroma_client.get_vectorstore()
        await chroma_client.add_documents(all_chunks)
    else:
        print("âš ï¸ No chunks to add for GDPR.")
    print("--- ğŸ END: GDPR Preprocessing ---\n")

async def run_custom_preprocessing_EUAI(csv: list[str]):
    print("\n--- ğŸš€ START: EU AI Act Preprocessing ---")
    all_chunks = []

    for csv_path in csv:
        if os.path.exists(csv_path):
            print(f"ğŸ“„ Processing: {csv_path}")
            chunks = await CustomPreprocessingEUAI.process_csv_to_chunks(csv_path)
            all_chunks.extend(chunks)
        else:
            print(f"âš ï¸ Warning: File not found: {csv_path}")

    if all_chunks:
        print(f"ğŸ“Š Total EU AI chunks generated: {len(all_chunks)}")
        collection_name = "gdpr_euAI_complainces_custom_preprocess"
        chroma_client = ChromadDBConfig(collection_name=collection_name)
        
        await chroma_client.get_vectorstore()
        await chroma_client.add_documents(all_chunks)
    else:
        print("âš ï¸ No chunks to add for EU AI Act.")
    print("--- ğŸ END: EU AI Act Preprocessing ---\n")

async def main():
    print("ğŸ› ï¸ DEBUG: Pipeline Script Started")
    
    # Verify Directory Structure
    print(f"ğŸ“‚ Current Working Directory: {os.getcwd()}")
    if os.path.exists("pipeline/pdfs"):
         print(f"ğŸ“‚ Files in pipeline/pdfs: {os.listdir('pipeline/pdfs')}")
    else:
         print("âŒ ERROR: 'pipeline/pdfs' directory does not exist!")

    pdfs = ["pipeline/pdfs/GDPR_policies.pdf", "pipeline/pdfs/EU AI Act.pdf"]
    jsonl = ["pipeline/pdfs/gdpr_articles_kaggle.jsonl"]
    csv = ["pipeline/pdfs/eu_ai_act_2024_from_pdf.csv"]

    await run_custom_preprocessing_EUAI(csv)
    await run_custom_preprocessing_gdpr(jsonl)
    await run_basic_preprocessing(pdfs)
    
    print("âœ… DEBUG: Pipeline Script Finished Successfully")

if __name__ == "__main__":
    asyncio.run(main())